# 第十四章、磁碟配額(Quota)與進階檔案系統管理

## 14.1 磁碟配額 (Quota) 的應用與實作

Quota 這個玩意兒就字面上的意思來看，就是有多少『限額』的意思啦！如果是用在零用錢上面， 就是類似『有多少零用錢一個月』的意思之類的。如果是在電腦主機的磁碟使用量上呢？以 Linux 來說，就是有多少容量限制的意思囉。我們可以使用 quota 來讓磁碟的容量使用較為公平， 底下我們會介紹什麼是 quota ，然後以一個完整的範例來介紹 quota 的實作喔！

### 14.1.1 什麼是 Quota

在 Linux 系統中，由於是多人多工的環境，所以會有多人共同使用一個硬碟空間的情況發生， 如果其中有少數幾個使用者大量的佔掉了硬碟空間的話，那勢必壓縮其他使用者的使用權力！ 因此管理員應該適當的限制硬碟的容量給使用者，以妥善的分配系統資源！避免有人抗議呀！

舉例來說，我們使用者的預設家目錄都是在 /home 底下，如果 /home 是個獨立的 partition ， 假設這個分割槽有 10G 好了，而 /home 底下共有 30 個帳號，也就是說，每個使用者平均應該會有 333MB 的空間才對。 偏偏有個使用者在他的家目錄底下塞了好多隻影片，佔掉了 8GB 的空間，想想看，是否造成其他正常使用者的不便呢？ 如果想要讓磁碟的容量公平的分配，這個時候就得要靠 quota 的幫忙囉！

#### Quota 的一般用途 (註1)

quota 比較常使用的幾個情況是：

針對 WWW server ，例如：每個人的網頁空間的容量限制！
針對 mail server，例如：每個人的郵件空間限制。
針對 file server，例如：每個人最大的可用網路硬碟空間 (教學環境中最常見！)
上頭講的是針對網路服務的設計，如果是針對 Linux 系統主機上面的設定那麼使用的方向有底下這一些：

限制某一群組所能使用的最大磁碟配額 (使用群組限制)：
你可以將你的主機上的使用者分門別類，有點像是目前很流行的付費與免付費會員制的情況， 你比較喜好的那一群的使用配額就可以給高一些！呵呵！ ^_^...

限制某一使用者的最大磁碟配額 (使用使用者限制)：
在限制了群組之後，你也可以再繼續針對個人來進行限制，使得同一群組之下還可以有更公平的分配！

限制某一目錄 (directory, project) 的最大磁碟配額：
在舊版的 CentOS 當中，使用的預設檔案系統為 EXT 家族，這種檔案系統的磁碟配額主要是針對整個檔案系統來處理，所以大多針對『掛載點』進行設計。 新的 xfs 可以使用 project 這種模式，就能夠針對個別的目錄 (非檔案系統喔) 來設計磁碟配額耶！超棒的！
大概有這些實際的用途啦！基本上，quota 就是在回報管理員磁碟使用率以及讓管理員管理磁碟使用情況的一個工具就是了！ 比較特別的是，XFS 的 quota 是整合到檔案系統內，並不是其他外掛的程式來管理的，因此，透過 quota 來直接回報磁碟使用率，要比 unix 工具來的快速！ 舉例來說， du 這東西會重新計算目錄下的磁碟使用率，但 xfs 可以透過 xfs_quota 來直接回報各目錄使用率，速度上是快非常多！

#### Quota 的使用限制

雖然 quota 很好用，但是使用上還是有些限制要先瞭解的：

在 EXT 檔案系統家族僅能針對整個 filesystem：
EXT 檔案系統家族在進行 quota 限制的時候，它僅能針對整個檔案系統來進行設計，無法針對某個單一的目錄來設計它的磁碟配額。 因此，如果你想要使用不同的檔案系統進行 quota 時，請先搞清楚該檔案系統支援的情況喔！因為 XFS 已經可以使用 project 模式來設計不同目錄的磁碟配額。

核心必須支援 quota ：
Linux 核心必須有支援 quota 這個功能才行：如果你是使用 CentOS 7.x 的預設核心， 嘿嘿！那恭喜你了，你的系統已經預設有支援 quota 這個功能囉！如果你是自行編譯核心的， 那麼請特別留意你是否已經『真的』開啟了 quota 這個功能？否則底下的功夫將全部都視為『白工』。

只對一般身份使用者有效：
這就有趣了！並不是所有在 Linux 上面的帳號都可以設定 quota 呢，例如 root 就不能設定 quota ， 因為整個系統所有的資料幾乎都是他的啊！ ^_^

若啟用 SELinux，非所有目錄均可設定 quota ：
新版的 CentOS 預設都有啟用 SELinux 這個核心功能，該功能會加強某些細部的權限控制！由於擔心管理員不小心設定錯誤，因此預設的情況下， quota 似乎僅能針對 /home 進行設定而已～因此，如果你要針對其他不同的目錄進行設定，請參考到後續章節查閱解開 SELinux 限制的方法喔！ 這就不是 quota 的問題了...
新版的 CentOS 使用的 xfs 確實比較有趣！不但無須額外的 quota 紀錄檔，也能夠針對檔案系統內的不同目錄進行配置！相當有趣！ 只是不同的檔案系統在 quota 的處理情況上不太相同，因此這裡要特別強調，進行 quota 前，先確認你的檔案系統吧！

#### Quota 的規範設定項目

quota 這玩意兒針對 XFS filesystem 的限制項目主要分為底下幾個部分：

分別針對使用者、群組或個別目錄 (user, group & project)：
XFS 檔案系統的 quota 限制中，主要是針對群組、個人或單獨的目錄進行磁碟使用率的限制！

容量限制或檔案數量限制 (block 或 inode)：
我們在第七章談到檔案系統中，說到檔案系統主要規劃為存放屬性的 inode 與實際檔案資料的 block 區塊，Quota 既然是管理檔案系統，所以當然也可以管理 inode 或 block 囉！ 這兩個管理的功能為：

限制 inode 用量：可以管理使用者可以建立的『檔案數量』；
限制 block 用量：管理使用者磁碟容量的限制，較常見為這種方式。
柔性勸導與硬性規定 (soft/hard)：
既然是規範，當然就有限制值。不管是 inode/block ，限制值都有兩個，分別是 soft 與 hard。 通常 hard 限制值要比 soft 還要高。舉例來說，若限制項目為 block ，可以限制 hard 為 500MBytes 而 soft 為 400MBytes。這兩個限值的意義為：

hard：表示使用者的用量絕對不會超過這個限制值，以上面的設定為例， 使用者所能使用的磁碟容量絕對不會超過 500Mbytes ，若超過這個值則系統會鎖住該用戶的磁碟使用權；
soft：表示使用者在低於 soft 限值時 (此例中為 400Mbytes)，可以正常使用磁碟，但若超過 soft 且低於 hard 的限值 (介於 400~500Mbytes 之間時)，每次使用者登入系統時，系統會主動發出磁碟即將爆滿的警告訊息， 且會給予一個寬限時間 (grace time)。不過，若使用者在寬限時間倒數期間就將容量再次降低於 soft 限值之下， 則寬限時間會停止。
會倒數計時的寬限時間 (grace time)：
剛剛上面就談到寬限時間了！這個寬限時間只有在使用者的磁碟用量介於 soft 到 hard 之間時，才會出現且會倒數的一個咚咚！ 由於達到 hard 限值時，使用者的磁碟使用權可能會被鎖住。為了擔心使用者沒有注意到這個磁碟配額的問題， 因此設計了 soft 。當你的磁碟用量即將到達 hard 且超過 soft 時，系統會給予警告，但也會給一段時間讓使用者自行管理磁碟。 一般預設的寬限時間為七天，如果七天內你都不進行任何磁碟管理，那麼 soft 限制值會即刻取代 hard 限值來作為 quota 的限制。

以上面設定的例子來說，假設你的容量高達 450MBytes 了，那七天的寬限時間就會開始倒數， 若七天內你都不進行任何刪除檔案的動作來替你的磁碟用量瘦身， 那麼七天後你的磁碟最大用量將變成 400MBytes (那個 soft 的限制值)，此時你的磁碟使用權就會被鎖住而無法新增檔案了。

整個 soft, hard, grace time 的相關性我們可以用底下的圖示來說明：

<div align=center><img src="os_note/hard_drive/images/soft_hard.gif"></div>
<div align=center>圖14.1.1、soft, hard, grace time 的相關性</div>

圖中的長條圖為使用者的磁碟容量，soft/hard 分別是限制值。只要小於 400M 就一切 OK ， 若高於 soft 就出現 grace time 並倒數且等待使用者自行處理，若到達 hard 的限制值， 那我們就搬張小板凳等著看好戲啦！嘿嘿！^_^！這樣圖示有清楚一點了嗎？

## 14.2 軟體磁碟陣列 (Software RAID)

在過去鳥哥還年輕的時代，我們能使用的硬碟容量都不大，幾十 GB 的容量就是大硬碟了！但是某些情況下，我們需要很大容量的儲存空間， 例如鳥哥在跑的空氣品質模式所輸出的資料檔案一個案例通常需要好幾 GB ，連續跑個幾個案例，磁碟容量就不夠用了。 此時我該如何是好？其實可以透過一種儲存機制，稱為磁碟陣列 (RAID) 的就是了。這種機制的功能是什麼？他有哪些等級？什麼是硬體、軟體磁碟陣列？Linux 支援什麼樣的軟體磁碟陣列？ 底下就讓我們來談談！

### 14.2.1 什麼是 RAID

磁碟陣列全名是『 Redundant Arrays of Independent Disks, RAID 』，英翻中的意思為：**獨立容錯式磁碟陣列，舊稱為容錯式廉價磁碟陣列**， 反正就稱為磁碟陣列即可！RAID 可以透過一個技術(軟體或硬體)，將多個較小的磁碟整合成為一個較大的磁碟裝置； 而這個較大的磁碟功能可不止是儲存而已，他還具有資料保護的功能呢。整個 RAID 由於選擇的等級 (level) 不同，而使得整合後的磁碟具有不同的功能，基本常見的 level 有這幾種(註2)：

#### RAID-0 (等量模式, stripe)：效能最佳

這種模式如果使用相同型號與容量的磁碟來組成時，效果較佳。這種模式的 RAID 會將磁碟先切出等量的區塊 (名為chunk，一般可設定 4K~1M 之間)， 然後當一個檔案要寫入 RAID 時，該檔案會依據 chunk 的大小切割好，之後再依序放到各個磁碟裡面去。由於每個磁碟會交錯的存放資料， 因此當你的資料要寫入 RAID 時，資料會被等量的放置在各個磁碟上面。舉例來說，你有兩顆磁碟組成 RAID-0 ， 當你有 100MB 的資料要寫入時，每個磁碟會各被分配到 50MB 的儲存量。RAID-0 的示意圖如下所示：

<div align=center><img src="/os_note/hard_drive/images/raid0.gif"></div>
<div align=center>圖14.2.1、RAID-0 的磁碟寫入示意圖</div>

上圖的意思是，在組成 RAID-0 時，每顆磁碟 (Disk A 與 Disk B) 都會先被區隔成為小區塊 (chunk)。 當有資料要寫入 RAID 時，資料會先被切割成符合小區塊的大小，然後再依序一個一個的放置到不同的磁碟去。 由於資料已經先被切割並且依序放置到不同的磁碟上面，因此每顆磁碟所負責的資料量都降低了！照這樣的情況來看， 越多顆磁碟組成的 RAID-0 效能會越好，因為每顆負責的資料量就更低了！ 這表示我的資料可以分散讓多顆磁碟來儲存，當然效能會變的更好啊！此外，磁碟總容量也變大了！ 因為每顆磁碟的容量最終會加總成為 RAID-0 的總容量喔！

**只是使用此等級你必須要自行負擔資料損毀的風險，由上圖我們知道檔案是被切割成為適合每顆磁碟分割區塊的大小， 然後再依序放置到各個磁碟中**。想一想，如果某一顆磁碟損毀了，那麼檔案資料將缺一塊，此時這個檔案就損毀了。 由於每個檔案都是這樣存放的，**因此 RAID-0 只要有任何一顆磁碟損毀，在 RAID 上面的所有資料都會遺失而無法讀取**。

另外，如果使用不同容量的磁碟來組成 RAID-0 時，由於資料是一直等量的依序放置到不同磁碟中，當小容量磁碟的區塊被用完了， 那麼所有的資料都將被寫入到最大的那顆磁碟去。舉例來說，我用 200G 與 500G 組成 RAID-0 ， 那麼最初的 400GB 資料可同時寫入兩顆磁碟 (各消耗 200G 的容量)，後來再加入的資料就只能寫入 500G 的那顆磁碟中了。 此時的效能就變差了，因為只剩下一顆可以存放資料嘛！

#### RAID-1 (映射模式, mirror)：完整備份

**這種模式也是需要相同的磁碟容量的，最好是一模一樣的磁碟啦！如果是不同容量的磁碟組成 RAID-1 時，那麼總容量將以最小的那一顆磁碟為主！這種模式主要是『讓同一份資料，完整的保存在兩顆磁碟上頭』**。舉例來說，如果我有一個 100MB 的檔案，且我僅有兩顆磁碟組成 RAID-1 時， 那麼這兩顆磁碟將會同步寫入 100MB 到他們的儲存空間去。 因此，整體 RAID 的容量幾乎少了 50%。由於兩顆硬碟內容一模一樣，好像鏡子映照出來一樣， **所以我們也稱他為 mirror 模式囉**～

<div align=center><img src="/os_note/hard_drive/images/raid1.gif"></div>
<div align=center>圖14.2.2、RAID-1 的磁碟寫入示意圖</div>

如上圖所示，一份資料傳送到 RAID-1 之後會被分為兩股，並分別寫入到各個磁碟裡頭去。 由於同一份資料會被分別寫入到其他不同磁碟，因此如果要寫入 100MB 時，資料傳送到 I/O 匯流排後會被複製多份到各個磁碟， 結果就是資料量感覺變大了！因此在大量寫入 RAID-1 的情況下，寫入的效能可能會變的非常差 (因為我們只有一個南橋啊！)。 好在如果你使用的是硬體 RAID (磁碟陣列卡) 時，磁碟陣列卡會主動的複製一份而不使用系統的 I/O 匯流排，效能方面則還可以。 如果使用軟體磁碟陣列，可能效能就不好了。

由於兩顆磁碟內的資料一模一樣，所以任何一顆硬碟損毀時，你的資料還是可以完整的保留下來的！ 所以我們可以說， RAID-1 最大的優點大概就在於資料的備份吧！不過由於磁碟容量有一半用在備份， 因此總容量會是全部磁碟容量的一半而已。雖然 RAID-1 的寫入效能不佳，不過讀取的效能則還可以啦！這是因為資料有兩份在不同的磁碟上面，如果多個 processes 在讀取同一筆資料時， RAID 會自行取得最佳的讀取平衡。

#### RAID 1+0，RAID 0+1

RAID-0 的效能佳但是資料不安全，RAID-1 的資料安全但是效能不佳，那麼能不能將這兩者整合起來設定 RAID 呢？ 可以啊！那就是 RAID 1+0 或 RAID 0+1。所謂的 RAID 1+0 就是： (1)先讓兩顆磁碟組成 RAID 1，並且這樣的設定共有兩組； (2)將這兩組 RAID 1 再組成一組 RAID 0。這就是 RAID 1+0 囉！反過來說，RAID 0+1 就是先組成 RAID-0 再組成 RAID-1 的意思。

<div align=center><img src="/os_note/hard_drive/images/raid1+0.jpg"></div>
<div align=center>圖14.2.3、RAID-1+0 的磁碟寫入示意圖</div>

如上圖所示，Disk A + Disk B 組成第一組 RAID 1，Disk C + Disk D 組成第二組 RAID 1， 然後這兩組再整合成為一組 RAID 0。如果我有 100MB 的資料要寫入，則由於 RAID 0 的關係， 兩組 RAID 1 都會寫入 50MB，又由於 RAID 1 的關係，因此每顆磁碟就會寫入 50MB 而已。 如此一來不論哪一組 RAID 1 的磁碟損毀，由於是 RAID 1 的映像資料，因此就不會有任何問題發生了！這也是目前儲存設備廠商最推薦的方法！

>為何會推薦 RAID 1+0 呢？想像你有 20 顆磁碟組成的系統，每兩顆組成一個 RAID1，因此你就有總共 10組可以自己復原的系統了！ 然後這 10組再組成一個新的 RAID0，速度立刻拉升 10倍了！同時要注意，因為每組 RAID1 是個別獨立存在的，因此任何一顆磁碟損毀， 資料都是從另一顆磁碟直接複製過來重建，並不像 RAID5/RAID6 必須要整組 RAID 的磁碟共同重建一顆獨立的磁碟系統！效能上差非常多！ 而且 RAID 1 與 RAID 0 是不需要經過計算的 (striping) ！讀寫效能也比其他的 RAID 等級好太多了！

#### RAID 5：效能與資料備份的均衡考量

RAID-5 至少需要三顆以上的磁碟才能夠組成這種類型的磁碟陣列。這種磁碟陣列的資料寫入有點類似 RAID-0 ， 不過每個循環的寫入過程中 (striping)，在每顆磁碟還加入一個同位檢查資料 (Parity) ，這個資料會記錄其他磁碟的備份資料， 用於當有磁碟損毀時的救援。RAID-5 讀寫的情況有點像底下這樣：

<div align=center><img src="/os_note/hard_drive/images/raid5.gif"></div>
<div align=center>圖14.2.4、RAID-5 的磁碟寫入示意圖</div>

如上圖所示，每個循環寫入時，都會有部分的同位檢查碼 (parity) 被記錄起來，並且記錄的同位檢查碼每次都記錄在不同的磁碟， 因此，任何一個磁碟損毀時都能夠藉由其他磁碟的檢查碼來重建原本磁碟內的資料喔！不過需要注意的是， 由於有同位檢查碼，因此 RAID 5 的總容量會是整體磁碟數量減一顆。以上圖為例， 原本的 3 顆磁碟只會剩下 (3-1)=2 顆磁碟的容量。而且當損毀的磁碟數量大於等於兩顆時，這整組 RAID 5 的資料就損毀了。 因為 RAID 5 預設僅能支援一顆磁碟的損毀情況。

在讀寫效能的比較上，讀取的效能還不賴！與 RAID-0 有的比！不過寫的效能就不見得能夠增加很多！ 這是因為要寫入 RAID 5 的資料還得要經過計算同位檢查碼 (parity) 的關係。由於加上這個計算的動作， 所以寫入的效能與系統的硬體關係較大！尤其當使用軟體磁碟陣列時，同位檢查碼是透過 CPU 去計算而非專職的磁碟陣列卡， 因此效能方面還需要評估。

另外，由於 RAID 5 僅能支援一顆磁碟的損毀，因此近來還有發展出另外一種等級，就是 RAID 6 ，這個 RAID 6 則使用兩顆磁碟的容量作為 parity 的儲存，因此整體的磁碟容量就會少兩顆，但是允許出錯的磁碟數量就可以達到兩顆了！ 也就是在 RAID 6 的情況下，同時兩顆磁碟損毀時，資料還是可以救回來！

#### Spare Disk：預備磁碟的功能：

當磁碟陣列的磁碟損毀時，就得要將壞掉的磁碟拔除，然後換一顆新的磁碟。換成新磁碟並且順利啟動磁碟陣列後， 磁碟陣列就會開始主動的重建 (rebuild) 原本壞掉的那顆磁碟資料到新的磁碟上！然後你磁碟陣列上面的資料就復原了！ 這就是磁碟陣列的優點。不過，我們還是得要動手拔插硬碟，除非你的系統有支援熱拔插，否則通常得要關機才能這麼做。

為了讓系統可以即時的在壞掉硬碟時主動的重建，因此就需要預備磁碟 (spare disk) 的輔助。 所謂的 spare disk 就是一顆或多顆沒有包含在原本磁碟陣列等級中的磁碟，這顆磁碟平時並不會被磁碟陣列所使用， 當磁碟陣列有任何磁碟損毀時，則這顆 spare disk 會被主動的拉進磁碟陣列中，並將壞掉的那顆硬碟移出磁碟陣列！ 然後立即重建資料系統。如此你的系統則可以永保安康啊！若你的磁碟陣列有支援熱拔插那就更完美了！ 直接將壞掉的那顆磁碟拔除換一顆新的，再將那顆新的設定成為 spare disk ，就完成了！

舉例來說，鳥哥之前所待的研究室有一個磁碟陣列可允許 16 顆磁碟的數量，不過我們只安裝了 10 顆磁碟作為 RAID 5。 每顆磁碟的容量為 250GB，我們用了一顆磁碟作為 spare disk ，並將其他的 9 顆設定為一個 RAID 5， 因此這個磁碟陣列的總容量為： (9-1)*250G=2000G。運作了一兩年後真的有一顆磁碟壞掉了，我們後來看燈號才發現！ 不過對系統沒有影響呢！因為 spare disk 主動的加入支援，壞掉的那顆拔掉換顆新的，並重新設定成為 spare 後， 系統內的資料還是完整無缺的！嘿嘿！真不錯！

#### 磁碟陣列的優點

說的口沫橫飛，重點在哪裡呢？其實你的系統如果需要磁碟陣列的話，其實重點在於：

資料安全與可靠性：指的並非網路資訊安全，而是當硬體 (指磁碟) 損毀時，資料是否還能夠安全的救援或使用之意；
讀寫效能：例如 RAID 0 可以加強讀寫效能，讓你的系統 I/O 部分得以改善；
容量：可以讓多顆磁碟組合起來，故單一檔案系統可以有相當大的容量。
尤其資料的可靠性與完整性更是使用 RAID 的考量重點！畢竟硬體壞掉換掉就好了，軟體資料損毀那可不是鬧著玩的！ 所以企業界為何需要大量的 RAID 來做為檔案系統的硬體基準，現在您有點瞭解了吧？那依據這三個重點，我們來列表看看上面幾個重要的 RAID 等級各有哪些優點吧！假設有 n 顆磁碟組成的 RAID 設定喔！

|項目|RAID0|RAID1|RAID10|RAID5|RAID6|
|---|---|---|---|---|---|
|最少磁碟數|2|2|4|3|4|
|最大容錯磁碟數(1)|無|n-1|n/2|1|2|
|資料安全性(1)|完全沒有|最佳|最佳|好|比 RAID5 好|
|理論寫入效能(2)|n|1|n/2|<n-1|<n-2|
|理論讀出效能(2)|n|n|n|<n-1|<n-2|
|可用容量(3)|n|1|n/2|n-1|n-2|
|一般應用|強調效能但資料不重要的環境|資料與備份|伺服器、雲系統常用|資料與備份|資料與備份|

註：因為 RAID5, RAID6 讀寫都需要經過 parity 的計算機制，因此讀/寫效能都不會剛好滿足於使用的磁碟數量喔！

另外，根據使用的情況不同，一般推薦的磁碟陣列等級也不太一樣。以鳥哥為例，在鳥哥的跑空氣品質模式之後的輸出資料，動輒幾百 GB 的單一大檔案資料， 這些情況鳥哥會選擇放在 RAID6 的陣列環境下，這是考量到資料保全與總容量的應用，因為 RAID 6 的效能已經足以應付模式讀入所需的環境。

近年來鳥哥也比較積極在作一些雲程式環境的設計，在雲環境下，確保每個虛擬機器能夠快速的反應以及提供資料保全是最重要的部份！ 因此效能方面比較弱的 RAID5/RAID6 是不考慮的，總結來說，大概就剩下 RAID10 能夠滿足雲環境的效能需求了。在某些更特別的環境下， 如果搭配 SSD 那才更具有效能上的優勢哩！

### 14.2.2 software, hardware RAID

為何磁碟陣列又分為硬體與軟體呢？所謂的硬體磁碟陣列 (hardware RAID) 是透過磁碟陣列卡來達成陣列的目的。 磁碟陣列卡上面有一塊專門的晶片在處理 RAID 的任務，因此在效能方面會比較好。在很多任務 (例如 RAID 5 的同位檢查碼計算) 磁碟陣列並不會重複消耗原本系統的 I/O 匯流排，理論上效能會較佳。此外目前一般的中高階磁碟陣列卡都支援熱拔插， 亦即在不關機的情況下抽換損壞的磁碟，對於系統的復原與資料的可靠性方面非常的好用。

不過一塊好的磁碟陣列卡動不動就上萬元台幣，便宜的在主機板上面『附贈』的磁碟陣列功能可能又不支援某些高階功能， 例如低階主機板若有磁碟陣列晶片，通常僅支援到 RAID0 與 RAID1 ，鳥哥喜歡的 RAID6 並沒有支援。 此外，作業系統也必須要擁有磁碟陣列卡的驅動程式，才能夠正確的捉到磁碟陣列所產生的磁碟機！

由於磁碟陣列有很多優秀的功能，然而硬體磁碟陣列卡偏偏又貴的很～因此就有發展出利用軟體來模擬磁碟陣列的功能， 這就是所謂的軟體磁碟陣列 (software RAID)。軟體磁碟陣列主要是透過軟體來模擬陣列的任務， 因此會損耗較多的系統資源，比如說 CPU 的運算與 I/O 匯流排的資源等。不過目前我們的個人電腦實在已經非常快速了， 因此以前的速度限制現在已經不存在！所以我們可以來玩一玩軟體磁碟陣列！

我們的 CentOS 提供的軟體磁碟陣列為 mdadm 這套軟體，這套軟體會以 partition 或 disk 為磁碟的單位，也就是說，你不需要兩顆以上的磁碟，只要有兩個以上的分割槽 (partition) 就能夠設計你的磁碟陣列了。此外， mdadm 支援剛剛我們前面提到的 RAID0/RAID1/RAID5/spare disk 等！ 而且提供的管理機制還可以達到類似熱拔插的功能，可以線上 (檔案系統正常使用) 進行分割槽的抽換！ 使用上也非常的方便呢！

另外你必須要知道的是，硬體磁碟陣列在 Linux 底下看起來就是一顆實際的大磁碟，因此硬體磁碟陣列的裝置檔名為 /dev/sd[a-p] ，因為使用到 SCSI 的模組之故。至於軟體磁碟陣列則是系統模擬的，因此使用的裝置檔名是系統的裝置檔， 檔名為 /dev/md0, /dev/md1...，兩者的裝置檔名並不相同！不要搞混了喔！因為很多朋友常常覺得奇怪， 怎麼他的 RAID 檔名跟我們這裡測試的軟體 RAID 檔名不同，所以這裡特別強調說明喔！

## 14.3 邏輯捲軸管理員 (Logical Volume Manager)

想像一個情況，你在當初規劃主機的時候將 /home 只給他 50G ，等到使用者眾多之後導致這個 filesystem 不夠大， 此時你能怎麼作？多數的朋友都是這樣：再加一顆新硬碟，然後重新分割、格式化，將 /home 的資料完整的複製過來， 然後將原本的 partition 卸載重新掛載新的 partition 。啊！好忙碌啊！若是第二次分割卻給的容量太多！導致很多磁碟容量被浪費了！ 你想要將這個 partition 縮小時，又該如何作？將上述的流程再搞一遍！唉～煩死了，尤其複製很花時間ㄟ～有沒有更簡單的方法呢？ 有的！那就是我們這個小節要介紹的 LVM 這玩意兒！

LVM 的重點在於『可以彈性的調整 filesystem 的容量！』而並非在於效能與資料保全上面。 需要檔案的讀寫效能或者是資料的可靠性，請參考前面的 RAID 小節。 LVM 可以整合多個實體 partition 在一起， 讓這些 partitions 看起來就像是一個磁碟一樣！而且，還可以在未來新增或移除其他的實體 partition 到這個 LVM 管理的磁碟當中。 如此一來，整個磁碟空間的使用上，實在是相當的具有彈性啊！ 既然 LVM 這麼好用，那就讓我們來瞧瞧這玩意吧！

### 14.3.1 什麼是 LVM： PV, PE, VG, LV 的意義

LVM 的全名是 Logical Volume Manager，中文可以翻譯作邏輯捲軸管理員。之所以稱為『捲軸』可能是因為可以將 filesystem 像捲軸一樣伸長或縮短之故吧！LVM 的作法是將幾個實體的 partitions (或 disk) 透過軟體組合成為一塊看起來是獨立的大磁碟 (VG) ，然後將這塊大磁碟再經過分割成為可使用分割槽 (LV)， 最終就能夠掛載使用了。但是為什麼這樣的系統可以進行 filesystem 的擴充或縮小呢？其實與一個稱為 PE 的項目有關！ 底下我們就得要針對這幾個項目來好好聊聊！

#### Physical Volume, PV, 實體捲軸

我們實際的 partition (或 Disk) 需要調整系統識別碼 (system ID) 成為 8e (LVM 的識別碼)，然後再經過 pvcreate 的指令將他轉成 LVM 最底層的實體捲軸 (PV) ，之後才能夠將這些 PV 加以利用！ 調整 system ID 的方是就是透過 gdisk 啦！

#### Volume Group, VG, 捲軸群組

所謂的 LVM 大磁碟就是將許多 PV 整合成這個 VG 的東西就是啦！所以 VG 就是 LVM 組合起來的大磁碟！這麼想就好了。 那麼這個大磁碟最大可以到多少容量呢？這與底下要說明的 PE 以及 LVM 的格式版本有關喔～在預設的情況下， 使用 32位元的 Linux 系統時，基本上 LV 最大僅能支援到 65534 個 PE 而已，若使用預設的 PE 為 4MB 的情況下， 最大容量則僅能達到約 256GB 而已～不過，這個問題在 64位元的 Linux 系統上面已經不存在了！LV 幾乎沒有啥容量限制了！

#### Physical Extent, PE, 實體範圍區塊

LVM 預設使用 4MB 的 PE 區塊，而 LVM 的 LV 在 32 位元系統上最多僅能含有 65534 個 PE (lvm1 的格式)，因此預設的 LVM 的 LV 會有 4M*65534/(1024M/G)=256G。這個 PE 很有趣喔！他是整個 LVM 最小的儲存區塊，也就是說，其實我們的檔案資料都是藉由寫入 PE 來處理的。簡單的說，這個 PE 就有點像檔案系統裡面的 block 大小啦。 這樣說應該就比較好理解了吧？所以調整 PE 會影響到 LVM 的最大容量喔！不過，在 CentOS 6.x 以後，由於直接使用 lvm2 的各項格式功能，以及系統轉為 64 位元，因此這個限制已經不存在了。

#### Logical Volume, LV, 邏輯捲軸

最終的 VG 還會被切成 LV，這個 LV 就是最後可以被格式化使用的類似分割槽的咚咚了！那麼 LV 是否可以隨意指定大小呢？ 當然不可以！既然 PE 是整個 LVM 的最小儲存單位，那麼 LV 的大小就與在此 LV 內的 PE 總數有關。 為了方便使用者利用 LVM 來管理其系統，因此 LV 的裝置檔名通常指定為『 /dev/vgname/lvname 』的樣式！

此外，我們剛剛有談到 LVM 可彈性的變更 filesystem 的容量，那是如何辦到的？其實他就是透過『交換 PE 』來進行資料轉換， 將原本 LV 內的 PE 移轉到其他裝置中以降低 LV 容量，或將其他裝置的 PE 加到此 LV 中以加大容量！ VG、LV 與 PE 的關係有點像下圖：

<div align=center><img src="/os_note/hard_drive/images/pe_vg.gif"></div>
<div align=center>圖14.3.1、PE 與 VG 的相關性圖示</div>

如上圖所示，VG 內的 PE 會分給虛線部分的 LV，如果未來這個 VG 要擴充的話，加上其他的 PV 即可。 而最重要的 LV 如果要擴充的話，也是透過加入 VG 內沒有使用到的 PE 來擴充的！